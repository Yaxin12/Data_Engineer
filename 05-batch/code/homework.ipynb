{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a71534-03bd-4e49-9624-15c54d47d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1. PySpark version (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cabeaf92-ed84-4b02-bab7-a0d4ddd6ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212ef69b-bf7f-482c-94bc-8d2e5665792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/05 19:29:58 WARN Utils: Your hostname, ZYX resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/05 19:29:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/05 19:29:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/05 19:29:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150714b5-5b9d-4dad-8fb0-e381ce1c6064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5aa7f0-69e8-4a0d-8e82-a1f901b6f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2. Size of partition (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916b3305-f040-48a1-afe6-1fb4ba514e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-05 19:32:15--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "3.161.178.218, 3.161.178.162, 3.161.178.24, ...vzurychx.cloudfront.net)... \n",
      "connected. to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.161.178.218|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61.36M  24.2MB/s    in 2.5s    \n",
      "\n",
      "2025-03-05 19:32:18 (24.2 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901cdcac-0e71-4918-9641-e38a5d908ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5ff311-7d83-4117-bb9b-3ec7dd5b80fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/05 19:39:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================>                                      (8 + 8) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/05 19:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================>                  (16 + 8) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/05 19:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Read the Parquet file\n",
    "df = spark.read.parquet('yellow_tripdata_2024-10.parquet')\n",
    "\n",
    "# Repartition the DataFrame into 24 partitions\n",
    "df = df.repartition(24)\n",
    "\n",
    "# Write the DataFrame to Parquet with compression\n",
    "df.write.parquet('data/pq/fhvhv/2021/02/', mode='overwrite', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "221925d8-27a0-4fb1-a1ac-f5313444748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()\n",
    "\n",
    "df = spark.read.parquet('yellow_tripdata_2024-10.parquet')\n",
    "df = df.repartition(4)\n",
    "df.write.parquet('homework/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3907d6-5650-4b99-b209-303947c898cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3. Number of trips (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "452d8827-c241-4bf5-bb9d-eff6757bab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  128909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('trips_data')  \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT count(1) FROM trips_data \n",
    "WHERE DATE(tpep_pickup_datetime) = '2024-10-15';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2db6c-3c76-41d3-9c2d-c9511bd3c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5: What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a0d944f-c309-4982-8a82-dd15d91d74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================>                     (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     trip_duration|\n",
      "+------------------+\n",
      "|162.61777777777777|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) / 3600 AS trip_duration\n",
    "FROM trips_data\n",
    "ORDER BY trip_duration DESC\n",
    "LIMIT 1;\n",
    "\"\"\").show()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb4bf2-bf9c-4896-a5da-20276521e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6: Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccee5779-0aec-44e2-8fe9-5c85bcc96583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:====================================>                     (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                Zone|num_trips|\n",
      "+--------------------+---------+\n",
      "|Governor's Island...|        1|\n",
      "|       Arden Heights|        2|\n",
      "|       Rikers Island|        2|\n",
      "|         Jamaica Bay|        3|\n",
      "| Green-Wood Cemetery|        3|\n",
      "|   Rossville/Woodrow|        4|\n",
      "|Charleston/Totten...|        4|\n",
      "|       West Brighton|        4|\n",
      "|Eltingville/Annad...|        4|\n",
      "|       Port Richmond|        4|\n",
      "|        Crotona Park|        6|\n",
      "|         Great Kills|        6|\n",
      "|     Mariners Harbor|        7|\n",
      "|Heartland Village...|        7|\n",
      "|Saint George/New ...|        9|\n",
      "|             Oakwood|        9|\n",
      "|       Broad Channel|       10|\n",
      "|New Dorp/Midland ...|       10|\n",
      "|         Westerleigh|       12|\n",
      "|     Pelham Bay Park|       12|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "lookup_df = spark.read.option(\"header\", \"true\").csv('taxi_zone_lookup.csv')\n",
    "\n",
    "lookup_df.registerTempTable('lookup')  \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT lookup.Zone , count(1) as num_trips FROM trips_data \n",
    "INNER JOIN lookup ON lookup.LocationID = trips_data.PULocationID\n",
    "GROUP BY lookup.Zone\n",
    "ORDER BY num_trips ASC;\n",
    "\"\"\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179df40-4f65-42bd-8e78-242feff5b5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
